# <span style="color:red"> Project Maintenance and Correction üõ†Ô∏è </span>

**We are currently in maintenance mode and actively working on improvements. Please bear with us while we make necessary corrections.**


<span style="color:red"> ------------------------------------------- </span>


# LSC-ncRNA : Large Scale Classification of non-coding RNA 

***LSC-ncRNA*** is the implementation of a sequence-based method that relies on the computation and selection of common ncRNA sequence motifs to provide a set of features for effectively and efficiently classifying ncRNA families using a supervised learning approach. 

## General use 

The method is divided into two main steps:  

- 1) [Motifs computation and selection](#motifs_computation_and_selection)  this is the first program to generate a file at the CSV format that contains vector representations of RNA sequences. 

- 2) [Classification](#fit_predict) this is the second program called with the csv file as input to fit the models and to classify new ncRNAs. 

  
# Details & Experimentation Pipeline 

<!-- Requirements --> 

<h3 id="requirements"> :hammer_and_pick: Requirements</h3> 

*   __`C++14`__ 

*   __`Python3 (at least python 3.6)`__ 

*   __`Biopython`__ 

*   __`Datatable`__ 

*   __`Scikit-learn`__ 

*   __`Pandas`__ 

*   __`Numpy`__ 

## A) Dataset preparation   

Download the ncRNA data, for example from Rfam (https://rfam.xfam.org/). In our case we used the following 3 datasets 

All the data set (including the split into train and test files) are available in the repo in `datasets/`.

If you want to recreate it from scratch, you can apply the following steps:

### Dataset 1: Rfam 14.1 seed sequences from 3,016 ncRNAs families 
available in : `datasets/Rfam_14.1_dataset.zip`

1) Download the from the ftp server on https://ftp.ebi.ac.uk/pub/databases/Rfam/14.1/Rfam.seed.gz 

2) Extract Rfam.seed.gz into the file Rfam.seed. 

Command: `gunzip Rfam.seed.gz`

Divide the file Rfam.seed into each family in a separate files, one for each RNA family, such that the file name for each family is the Rfam family name (as Rfxxxxx  such that (xxxxx: is a number). 

You need to use the C++ code in (RNAFamilies_Stockholm_SeedAlignment_To_PlainFastaFiles), and compile it if necessary:

Command: `g++ main.cpp -o extract -std=c++14`

After that just use :

Command: `./extract -in path_to/Rfam.seed -out path_to/Rfam_out_files`


3) Change the sequence identifiers in the fasta file to the family name `>Rfamxxxxx` , so that sequence classes are provided for subsequent processing. 

This is done automatically when we [construct train test files](#train_test_label) which is explained bellow.
  

The first dataset is split into 30% Test and 70% Train. 

The train part  is used to do cross validation experimentation. We generate a sample with 600 families, and a sample with 350 families, and each of them is also split into 30% test and 70% train. 

For command explanation see : [construct train test files](#train_test_label) bellow.
  

### Dataset 2: 88 Rfam families with noise 

***Initial dataset*** : Download the data from https://github.com/bioinformatics-sannio/ncrna-deep. This benchmark dataset is used in the method deepncRNA in the paper "Deep learning predicts short non-coding RNA functions from only raw sequence data" ([paper link](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008415)). 

  

The dataset includes 306,016 sequences distributed among 88 RFAM families of small ncRNA such that each family contains more than 400 sequences.  Each dataset class was split into three random subsets for train (84\%), validation (8\%), and test (8\%) in the deepncRNA method. We kept the initial partition of the dataset to be able to compare our results with those of deepncRNA. 

  

***Noise dataset*** : For each sequence in the test dataset, we added x\% noise at its start and its end, i.e., x/2\% at each extremity, such that the percentage x\% of noise compared to the length of the original sequence varied between 0\% and 200\% with steps of 25\%. The noise was generated by randomly shuffling the sequence while preserving the nucleotide and dinucleotide frequency of the original sequence. 

  

  

### Dataset 3: Clans from Rfam 14.8 

***Low sequence similarity dataset*** : the  dataset consists of $36$ classes defined as the clans of RFAM 14.8 containing at least $4$ ncRNA families (https://rfam.xfam.org/clan/). This results in $36$ clans containing $4$ to $11$ families per clan and a total of $199$ families for all $36$ clans. For each clan,  $12$ sequences are selected from each family. 

  

***The high sequence similarity dataset*** : the  dataset consists of $199$ classes defined as the $199$ ncRNA families contained in the $36$ clans from the low sequence similarity dataset. 

  

*** List of used clans: *** : *["CL00051", "CL00003", "CL00069", "CL00106", "CL00038", "CL00054", "CL00002", "CL00014", "CL00102", "CL00001", "CL00117", "CL00021", "CL00057", "CL00111", "CL00118", "CL00066", "CL00112", "CL00116", "CL00010", "CL00005", "CL00027", "CL00053", "CL00063", "CL00100", "CL00119", "CL00004", "CL00032", "CL00034", "CL00035", "CL00040", "CL00096", "CL00093", "CL00015", "CL00121", "CL00036", "CL00045"]* 

  

Each class of each dataset was split into two subsets for train (70\%) and test (30\%). 

  

We use the script in ***dataset/clans_dataset.py*** to download and prepare the dataset. 


### <a name="train_test_label"> Dataset Train Test split and dataset statistics: 

  

We use ***constructTrainTestFiles*** to split the dataset into train and test, and to get the dataset statistics. 

- To split the dataset, for each family (fasta file), we split the sequences in  into train and test file. 

- dataset statistics: for a given folder, for each family (in separate file), we get the following information: nb seqs, min seq len, max seq len, average seq len , and save to csv file. 

  

The program can be use like this: 

constructTrainTestFiles -in \<string\> [-out \<string\> -nf \<integer\> -mins \<integer\> -maxs \<integer\> -pt \<integer\> -m \<string\>] 

  

- -in : \<string\> a path directory for fasta files 

- -out : \<string\> a main path directory the output results 

- -nf : \<integer\> number of families 

- -mins : \<integer\>, min number of sequences (default 4) 

- -maxs : \<integer\>, max number of sequences 

- -pt : \<integer\>, percentage of Test sequences, default value is 30. 

- -m : \<string\> user mode, several mode are available:  

    - -m i: (information) get all information as nb seqs, min seq len, max seq len, average seq len , and save to csv file. 

    - -m s: (sample dataset) get n random families that have nb seqs between min and max, and save them to dir_output 

    - -m sttmm : (split Train Test Min Max) for a given nb of families, and min max number of seqs, split into train and test data 

    - -m sttm : (split Train Test Min) consider only min number of seqs, and split all files in input folder into train and test data 

    - -m stt : (split Train Test) for all files in input folder, split into train and test data 

  

The output will be 1) a folder Train with *percentage_nb_seqs_train* sequences, 2) a folder Test  with *100%-percentage_nb_seqs_train* sequences, and 3) a csv file that contains the information. 

  

**note**, be aware of problem of file encoding, and endline. In our case, we have to change to encoding to UTF8 to avoid extra-space when reading a line (see my answer: https://stackoverflow.com/a/73952980/3429103) 

  

## <a name="motifs_computation_and_selection"> B) Motifs computation and selection: 

This is the first step in the method which is the computation and the selection of sequence motifs that allow defining a vectorial representation of ncRNAs sequences. The result is a 2d vector (matrix), that contains the number of occurrences for each motif that exist in the ncRNA sequence. The matrix is saved as a single csv file. 

  

The program can be used like this: 

  

./MatrixCmsNStrNbOccs -in \<string\> [-nf \<integer\> -mins \<integer\> -maxs \<integer\> -minl \<integer\> -maxl \<integer\> -d \<integer\> -b \<integer\> -a \<integer\> -g \<integer\> -tn \<string\>] 

  

  

- *-in* : \<string\> a path directory for fasta files 

- *-tn* : \<string\> experiment name, give a specific name to you your experiment, (default "test") 

- *-nf* : \<integer\> number of families, (**default 10**) 

- *-mins* : \<integer\>, min number of sequences, (**default 4**) 

- *-maxs* : \<integer\>, max number of sequences, (**default 1000**) 

- *-minl* : \<integer\>, min length of motif, (**default 2**) 

- *-maxl* : \<integer\>, max length of motif, (**default 10**) 

- *-d* : \<integer\> (0: false, 1 or other: true), delete sub-motifs, (**default 0**) 

- *-b* : \<integer\> beta (between [0 and 100]), (**default 40**) 

- *-a* : \<integer\>, alpha  (-1 no alpha, or 0,1,2,3,..., , (**default -1**) 

- *-g* : \<integer\> ( >=1), gamma, number of occurrences allowed, (**default 1**) 

  

All the parameters between [] are optional. The path to the directory of fasta files is mandatory. The motifs length (minl, maxl), Beta and experiment name parameters are recommended to use. 

  

Example: 

```shell 

nohup ./MatrixCmsNStrNbOccs -in "/data/ibra/Rfam_14.1_dataset/Rfam14.1_Sample_Train_Test/Rfam_600_Train_Test/Train" -minl 2 -maxl 8 -b 50 -g 1 -tn F_600 > out_F_600 & 

``` 

  

The output csv file name is as follows: del_[No/Yes:-d]_nbF_[test_name:-tn]_min_[-minl]_max_[-maxl]_beta_[-b]_alpha_[-a]_nbOccrs_[-g].csv. For exameple, the previous command produce the following name: del_No_nbF_F_600_min_2_max_8_beta_50_alpha_-1_nbOccrs_1.csv 

  

  

## C) Training and Testing experiments: 

  

This is the step of the selection of classification algorithms that allow to achieve the most accurate classification of ncRNA sequences. 

  

## C.1) selection of classification algorithms: 

To choose the best classification algorithm we use the following python program ***Classification/modelstest.py*** as follows: 

`python3 modelstest.py path_motifs.csv`. The code test the following algorithm ['ext','knn','rdf','gnb', 'dt', 'nlp', 'svc'] using 10-fold cross-validation with a split of 70% for train and 30% for validation. 

  

## <a name="fit_predict"> C.2) Experiments with the chosen models. 

  

To launcn the program we use the main python script as follow: 

*python3 Main.py mod "path/2d_matrix.csv" "path_test_folder"*, where: 

- mod is "EXT" (Extra-tree), "RDF" (Random forest), "NLP" (MultiLayer perceptron) and "VOT" (voting model).  

- path/2d_matrix.csv: the path of the csv matrix generated using the previous program. 

- path_test_folder: test folder, that contain a set of fasta files. 

  

Example: 

  

```shell 

python3 Main.py EXT "/data/ibra/del_No_nbF_F_600_min_6_max_7_beta_50_alpha_-1_nbOccrs_1.csv" "/data/ibra/Rfam_14.1_dataset/Rfam14.1_Sample_Train_Test/Rfam_600_Train_Test/Test" > res_EXT_Single_del_No_nbF_Clans_min_6_max_7_beta_50_alpha_-1_nbOccrs_1 

``` 

  

The training and test experiments generate  different scores: 

- Processing time for 1) training and 2) testing. 

- Scores of training `score Train` and prediction `pred Train`. 

- Accuracy, Precision, Recall, and fbeta_score. 

  

  

  

## D) Blast based classification (blastn(av-s)): 

We use Blast and add post-processing of its results to produce classification. We use a script to do the following steps: 

- a) From the Train folder: gather all sequences in all files in fasta file X. 

- b) use blast to create a database on this single file X (that contains all Train seqs) 

- c) Gather all sequences in Test folder in one single file Y. 

- d) use blast to search sequences in Y against the database for X. 

- e) Process blast result best hit to generate the classification. 

  

To lunch the script `blast_classification.py`, we need to give as argument the path for Train and Test folder, and the experiment name as shown in the following example: 

```shell 

nohup python blast_classification.py "/data/chei2402/ibra/test_infernal/Clans_family_train_test/Train/" "/data/chei2402/ibra/test_infernal/Clans_family_train_test/Test/" Clans > res_blast_class_Clans_time_acc & 

``` 

  

  

## D) Infernal based classification: 

  

  

### supplementary sub-folder:  

contains supplementary information and data for our paper. 

  

  

:busts_in_silhouette: __Authors__ 

`ibrahim Chegrane & Nabil Bendjaa & Aida Ouangraoua`, CoBIUS LAB, Department of Computer Science, Faculty of Science, Universit√© de Sherbrooke, Sherbrooke, Canada 

  

> :bulb: If you are using our algorithm in your research, please cite our recent paper: __Upcoming__  

  

> :e-mail: Contact: Aida[dot]Ouangraoua[at]usherbrooke[dot]ca 

 

#############################################

In a Git repository, it's a good practice to document the structure and purpose of files and directories to help contributors understand and navigate the project. You have a couple of options for where to put this information:

### Option 1: Include in README.md

The most common and straightforward approach is to include this information in the `README.md` file. This file is typically the first thing people see when they visit your repository, making it an ideal place for an overview of the project structure.

# Project Directory Structure
This section provide an provides an overview of the structure and purpose of files and directories in the repository.


```
LSC-ncRNA/
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Clans_ncRNA_from_Rfam_14.8/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deep_ncrna_datasets/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Rfam_14.1_dataset/
‚îÇ   ‚îú‚îÄ‚îÄ preparation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clans_dataset.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RNAFamilies_Stockholm_SeedAlignment_To_PlainFastaFiles/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ constructTrainTestFiles/
‚îÇ   ‚îî‚îÄ‚îÄ information/
‚îÇ       ‚îú‚îÄ‚îÄ All_Rfam_14point1_seed_ncRNA_Families_infos.xlsx
‚îÇ       ‚îú‚îÄ‚îÄ compute_scaling_distribution_dataset_sample.csv
‚îÇ       ‚îú‚îÄ‚îÄ compute_scaling_distribution_dataset_sample.xlsx
‚îú‚îÄ‚îÄ my_method/  
‚îÇ   ‚îú‚îÄ‚îÄ MotifsExtractionSelection/
‚îÇ   ‚îî‚îÄ‚îÄ Classification/
‚îú‚îÄ‚îÄ compared_methods/  
‚îÇ   ‚îú‚îÄ‚îÄ blast_classification.py
‚îÇ   ‚îî‚îÄ‚îÄ infernal/
‚îÇ       ‚îú‚îÄ‚îÄ testInfernal/
‚îÇ       ‚îî‚îÄ‚îÄ ParseResultInfernal/
‚îî‚îÄ‚îÄ supplementary/
```

## datasets/
- Contains all datasets, dataset preparation scripts, and dataset information.

### data/
Prepared datasets, it is ready to use. just unzip it.
- **Clans_ncRNA_from_Rfam_14.8/**: Prepared clan datasets.
- **deep_ncrna_datasets/**: Unzipped deep ncRNA datasets.
- **Rfam_14.1_dataset/**: Unzipped Rfam 14.1 datasets.

### preparation/
- **clans_dataset.py**: Script to prepare the clan dataset.
- **RNAFamilies_Stockholm_SeedAlignment_To_PlainFastaFiles/**: Scripts for seed alignment to plain FASTA files.
- **constructTrainTestFiles/**: Scripts for constructing train and test files.

### information/
- **All_Rfam_14point1_seed_ncRNA_Families_infos.xlsx**: Information about the Rfam 14.1 seed ncRNA families.
- **compute_scaling_distribution_dataset_sample.csv**: Dataset scaling information.
- **compute_scaling_distribution_dataset_sample.xlsx**: Dataset scaling information (Excel format).

## my_method/
- Contains scripts for main methods: motif extraction and classification.

### MotifsExtractionSelection/
- Scripts for motif extraction and selection.

### Classification/
- Scripts for classification.

## compared_methods/
- Contains scripts for compared methods: BLAST and Infernal.

### blast_classification.py
- Script for the BLAST classification method.

### infernal/
- **testInfernal/**: Testing scripts for Infernal.
- **ParseResultInfernal/**: Scripts for parsing Infernal results.

## supplementary/
- Additional supplementary materials.


# chatgpt 4.o proposition help

To create a clean, understandable, and professional README focused on your method, it is essential to structure it in a way that guides the user through the core aspects of your project: its purpose, how to use it, and any necessary requirements. Here's a suggested restructured README:

---

# LSC-ncRNA: Large Scale Classification of non-coding RNA

**LSC-ncRNA** is a sequence-based method designed to effectively and efficiently classify non-coding RNA (ncRNA) families using supervised learning. The method involves two main steps: motifs computation and selection, followed by classification.

## Table of Contents

1. [Overview](#overview)
2. [Requirements](#requirements)
3. [Usage](#usage)
    - [Motifs Computation and Selection](#motifs-computation-and-selection)
    - [Classification](#classification)
4. [Datasets](#datasets)
5. [Project Directory Structure](#project-directory-structure)
6. [Authors](#authors)
7. [Citation](#citation)

## Overview

The LSC-ncRNA method consists of:

1. **Motifs Computation and Selection**: Generates a CSV file containing vector representations of RNA sequences based on common sequence motifs.
2. **Classification**: Uses the generated CSV file to fit models and classify new ncRNAs.

## Requirements

- `C++14`
- `Python 3` (at least Python 3.6)
- `Biopython`
- `Datatable`
- `Scikit-learn`
- `Pandas`
- `Numpy`

## Usage

### Motifs Computation and Selection

1. **Prepare Dataset**
    - Ensure your datasets are available in the `datasets/data/` directory.
    - You can prepare the dataset from scratch using scripts in the `datasets/preparation/` directory if needed.

2. **Compute and Select Motifs**

    ```sh
    ./MatrixCmsNStrNbOccs -in <path_to_fasta_files> -minl 2 -maxl 8 -b 50 -g 1 -tn <experiment_name>
    ```

    **Example:**

    ```sh
    nohup ./MatrixCmsNStrNbOccs -in "/data/ibra/Rfam_14.1_dataset/Train" -minl 2 -maxl 8 -b 50 -g 1 -tn F_600 > out_F_600 &
    ```

    The output CSV file will have a name following this format: `del_[No/Yes:-d]_nbF_[test_name:-tn]_min_[-minl]_max_[-maxl]_beta_[-b]_alpha_[-a]_nbOccrs_[-g].csv`.

### Classification

1. **Select Classification Algorithms**

    Use `modelstest.py` to select the best classification algorithm.

    ```sh
    python3 modelstest.py path_to_motifs.csv
    ```

2. **Fit and Predict**

    Use `Main.py` to train the model and classify ncRNA sequences.

    ```sh
    python3 Main.py <model> <path_to_motifs_csv> <path_to_test_folder>
    ```

    **Example:**

    ```sh
    python3 Main.py EXT "/data/ibra/del_No_nbF_F_600_min_6_max_7_beta_50_alpha_-1_nbOccrs_1.csv" "/data/ibra/Rfam_14.1_dataset/Test" > res_EXT_Single_del_No_nbF_Clans_min_6_max_7_beta_50_alpha_-1_nbOccrs_1
    ```

    Supported models are "EXT" (Extra-tree), "RDF" (Random forest), "NLP" (MultiLayer perceptron), and "VOT" (voting model).

## Datasets

Datasets are pre-prepared and available in the `datasets/data/` directory. If you need to recreate the datasets from scratch, use the scripts in `datasets/preparation/`.

## Project Directory Structure

```
LSC-ncRNA/
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Clans_ncRNA_from_Rfam_14.8/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deep_ncrna_datasets/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Rfam_14.1_dataset/
‚îÇ   ‚îú‚îÄ‚îÄ preparation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clans_dataset.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RNAFamilies_Stockholm_SeedAlignment_To_PlainFastaFiles/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ constructTrainTestFiles/
‚îÇ   ‚îî‚îÄ‚îÄ information/
‚îÇ       ‚îú‚îÄ‚îÄ All_Rfam_14point1_seed_ncRNA_Families_infos.xlsx
‚îÇ       ‚îú‚îÄ‚îÄ compute_scaling_distribution_dataset_sample.csv
‚îÇ       ‚îú‚îÄ‚îÄ compute_scaling_distribution_dataset_sample.xlsx
‚îú‚îÄ‚îÄ my_method/  
‚îÇ   ‚îú‚îÄ‚îÄ MotifsExtractionSelection/
‚îÇ   ‚îî‚îÄ‚îÄ Classification/
‚îú‚îÄ‚îÄ compared_methods/  
‚îÇ   ‚îú‚îÄ‚îÄ blast_classification.py
‚îÇ   ‚îî‚îÄ‚îÄ infernal/
‚îÇ       ‚îú‚îÄ‚îÄ testInfernal/
‚îÇ       ‚îî‚îÄ‚îÄ ParseResultInfernal/
‚îî‚îÄ‚îÄ supplementary/
```

### datasets/
- **data/**: Contains prepared datasets.
- **preparation/**: Scripts for dataset preparation.
- **information/**: Information about datasets.

### my_method/
- **MotifsExtractionSelection/**: Scripts for motifs extraction and selection.
- **Classification/**: Scripts for classification.

### compared_methods/
- **blast_classification.py**: Script for BLAST classification method.
- **infernal/**: Scripts for Infernal method.

### supplementary/
- Additional supplementary materials.

## Authors

- Ibrahim Chegrane
- Nabil Bendjaa
- Aida Ouangraoua

CoBIUS LAB, Department of Computer Science, Faculty of Science, Universit√© de Sherbrooke, Sherbrooke, Canada

## Citation

If you are using our algorithm in your research, please cite our upcoming paper.

For any inquiries, contact: [Aida Ouangraoua](mailto:Aida.Ouangraoua@usherbrooke.ca)

---

This version of the README is focused on clarity and ease of use, highlighting the primary components and steps necessary to use your method while providing additional sections for dataset preparation and the project structure. This should help users understand and navigate your project more efficiently.


# script 

Sure! Here's a Python script that automates the entire process, from dataset preparation to motif computation, selection, and classification. This script assumes you have all the necessary tools and libraries installed and available in your environment.

```python
import os
import subprocess
import sys

def run_command(command):
    print(f"Running command: {command}")
    result = subprocess.run(command, shell=True)
    if result.returncode != 0:
        sys.exit(f"Command failed: {command}")

def prepare_dataset_rfam14_1(input_path, output_path):
    # Step 1: Download and extract Rfam.seed.gz
    os.makedirs(output_path, exist_ok=True)
    os.chdir(output_path)
    run_command(f"wget {input_path}")
    run_command("gunzip Rfam.seed.gz")

    # Step 2: Divide the file into separate files for each family
    run_command("g++ RNAFamilies_Stockholm_SeedAlignment_To_PlainFastaFiles/main.cpp -o extract -std=c++14")
    run_command(f"./extract -in Rfam.seed -out {output_path}/Rfam_out_files")

    # Step 3: Modify sequence identifiers and split the data into training and testing sets
    run_command(f"python3 constructTrainTestFiles.py -in {output_path}/Rfam_out_files -out {output_path} -m stt")

def compute_motifs(input_dir, min_length, max_length, beta, gamma, experiment_name):
    # Compute and select sequence motifs
    command = f"./MatrixCmsNStrNbOccs -in {input_dir} -minl {min_length} -maxl {max_length} -b {beta} -g {gamma} -tn {experiment_name}"
    run_command(command)

def select_classification_algorithm(csv_path):
    # Select the best classification algorithm
    run_command(f"python3 modelstest.py {csv_path}")

def train_and_test_model(model, csv_path, test_folder):
    # Train and test the model
    command = f"python3 Main.py {model} \"{csv_path}\" \"{test_folder}\""
    run_command(command)

if __name__ == "__main__":
    # Define paths and parameters
    rfam_input_path = "https://ftp.ebi.ac.uk/pub/databases/Rfam/14.1/Rfam.seed.gz"
    rfam_output_path = "datasets/data/Rfam_14.1_dataset"
    motifs_input_dir = f"{rfam_output_path}/Train"
    motifs_min_length = 2
    motifs_max_length = 8
    motifs_beta = 50
    motifs_gamma = 1
    experiment_name = "F_600"
    model = "EXT"  # Example model: EXT, RDF, NLP, etc.
    csv_path = f"{rfam_output_path}/del_No_nbF_F_600_min_{motifs_min_length}_max_{motifs_max_length}_beta_{motifs_beta}_alpha_-1_nbOccrs_{motifs_gamma}.csv"
    test_folder = f"{rfam_output_path}/Test"

    # Run the steps
    prepare_dataset_rfam14_1(rfam_input_path, rfam_output_path)
    compute_motifs(motifs_input_dir, motifs_min_length, motifs_max_length, motifs_beta, motifs_gamma, experiment_name)
    select_classification_algorithm(csv_path)
    train_and_test_model(model, csv_path, test_folder)
```

### Explanation

1. **`run_command` Function**: This function is used to execute shell commands and handle errors.
2. **`prepare_dataset_rfam14_1` Function**: This function handles the dataset preparation:
   - Downloads and extracts the Rfam data.
   - Compiles and runs the C++ program to split the data.
   - Modifies sequence identifiers and splits the dataset into training and testing sets using a Python script.
3. **`compute_motifs` Function**: This function computes and selects sequence motifs.
4. **`select_classification_algorithm` Function**: This function selects the best classification algorithm.
5. **`train_and_test_model` Function**: This function trains and tests the selected model.
6. **`__main__` Block**: This block sets paths and parameters, and runs all the steps sequentially.

### Assumptions
- The C++ code for `RNAFamilies_Stockholm_SeedAlignment_To_PlainFastaFiles` is located at the specified path and named `main.cpp`.
- The Python script `constructTrainTestFiles.py` exists and performs the dataset splitting.
- The `MatrixCmsNStrNbOccs` executable is available in the current working directory.

Make sure to adjust paths and parameters as necessary for your specific environment and data.



# Compilation and configuration
  
## Python environment

We use python3 , at least python 3.6

### Create a virtual environment

- 1. Create a virtual environment
```bash
python -m venv rna_env
```

- 2. Activate the virtual environment
  - On Unix or MacOS:
```bash
source rna_env/bin/activate
```
  - On Windows:
```bash
rna_env\Scripts\activate
```

- 3. Upgrade pip
```bash
python -m pip install --upgrade pip
```

- 4. Install the required packages
```bash
pip install -r requirements.txt
```

- 5. Deactivate the virtual environment
```bash
deactivate
```

### Install the required packages

All the required packages are in the `requirements.txt` file. To install the required packages, you can use the following command:
```bash
pip install -r requirements.txt
```

The individual packages used in this project are :
install individually:
```bash
pip install <package_name>
```

- `biopython` : https://biopython.org/wiki/Download
- `datatable` : https://datatable.readthedocs.io/en/latest/start/install.html 
  on windows and linux, the package worked fine for me after the installation using thecommnd `pip install datatable`. But on mac os, i had to install the package using the command `pip install git+https://github.com/h2oai/datatable` (build from source).
- `scikit-learn` : https://scikit-learn.org/stable/install.html 
- `pandas` : https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html
- `numpy` : https://numpy.org/install/
- `portalocker` : https://pypi.org/project/portalocker/ , to install it use `pip install "portalocker[redis]"`
- `xgboost` : https://xgboost.readthedocs.io/en/latest/install.html  this is not necessary, We didn't use this model in our study.
-     + `brew install libomp` for mac os, we need to install libomp, it is required by xgboost.
- `pyahocorasick` : https://pypi.org/project/pyahocorasick/
- `suffix-trees` : https://pypi.org/project/suffix-trees/




## C++

  __`C++14`__
  


